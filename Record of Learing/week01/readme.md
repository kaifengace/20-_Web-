# ğŸ¸PythonWebDM-Week02ç¬”è®°

---

## ğŸ“–æœ¬å‘¨ä¸»è¦å†…å®¹

### ğŸ›1.requests-htmlæ¨¡å—

requests-htmlæ¨¡å—æ˜¯ç±»ä¼¼requestsæ¨¡å—çš„è¯·æ±‚æ¨¡å—ï¼Œåœ¨è¯·æ±‚çš„åŸºç¡€ä¸Šå¯ä»¥é€šè¿‡htmlæ ‡ç­¾(xpath)è¿›è¡Œé€‰æ‹©çˆ¬å–æƒ³è¦çš„å†…å®¹ï¼Œå¯ä»¥å¸¦uaè®¿é—®ã€‚

æ­¤æ¨¡å—å¯ä»¥è®©æˆ‘ä»¬æ›´æ–¹ä¾¿çš„è§£æhtmlé¡µé¢ï¼Œä½¿å¾—çˆ¬è™«æ›´åŠ ç®€å•æ˜“æ‡‚ã€‚

ä½¿ç”¨æ–¹æ³•è¯¦è§[å®˜æ–¹æ–‡æ¡£](https://requests-html.kennethreitz.org/)ã€‚

<b>[ä¸­æ–‡æ–‡æ¡£](https://cncert.github.io/requests-html-doc-cn/#/)</b>

### ğŸ¤¯æ¥ç©ç©æ¨¡å—

ğŸ‘è¯•è¯•çœ‹æ¨¡å—çš„å¼ºå¤§

#### é€šè¿‡cssæ¥å®šä½

``` 

from requests_html import HTMLSession

session = HTMLSession()

r = session.get("https://news.cnblogs.com/n/recommend")

# é€šè¿‡CSSæ‰¾åˆ°æ–°é—»æ ‡ç­¾
news = r.html.find('h2.news_entry > a')

for new in news:
    print(new.text)  # è·å¾—æ–°é—»æ ‡é¢˜
    print(new.absolute_links)  # è·å¾—æ–°é—»é“¾æ¥

 ```

è¾“å‡ºç»“æœä¸ºï¼š

```
ã€ŠåŠ¨ç‰©æ£®å‹ä¼šã€‹åˆ·çˆ†æœ‹å‹åœˆçš„èƒŒå æ˜¯é€ƒç¦»æ—¶ä»£çš„æˆ‘ä»¬
{'https://news.cnblogs.com/n/658292/'}
ä¸€ç¾¤æ—¥æœ¬å°å­¦ç”Ÿï¼Œåœ¨ã€Šæˆ‘çš„ä¸–ç•Œã€‹é‡Œæäº†åœºâ€œäº‘â€æ¯•ä¸š
{'https://news.cnblogs.com/n/658276/'}
è¿™æ¬¾è¶…ç¡¬æ ¸çš„æ¸¸æˆï¼Œè•´è—ç€å¾®è½¯å®ç°å¤å…´çš„å·¨å¤§é‡å¿ƒ
{'https://news.cnblogs.com/n/658219/'}
Google å¼€æº Pigweedï¼Œæ¶‰è¶³åµŒå…¥å¼å¼€å‘
{'https://news.cnblogs.com/n/658198/'}
å¸®ç½—æ°¸æµ©ç®—ç¬”è´¦ï¼šåˆ›ä¸šè¿™äº›å¹´ï¼Œåˆ°åº•æŒ£äº†å¤šå°‘é’±ï¼Ÿ
{'https://news.cnblogs.com/n/658176/'}

......
......

```

#### é€šè¿‡Xpathå®šä½


```
from requests_html import HTMLSession

session = HTMLSession()

r = session.get("https://www.liepin.com/zhaopin/?key=web+mining")

# é€šè¿‡xpathæ‰¾åˆ°å·¥ä½œæ ‡ç­¾
news = r.html.xpath('//div[@class="job-info"]/h3/a')
for new in news:
    print(new.text)  # è·å¾—å·¥ä½œæ ‡é¢˜
    print(new.absolute_links)  # è·å¾—å·¥ä½œé“¾æ¥

```

è¾“å‡ºç»“æœä¸ºï¼š 

```
web mining Engineer
{'https://www.liepin.com/job/1917951351.shtml'}
( Seniorï¼‰ Research Manager
{'https://www.liepin.com/job/1923198421.shtml'}
(Associateï¼‰research Director
{'https://www.liepin.com/job/1920754473.shtml'}
é«˜çº§æ•°æ®åˆ†æå‘˜(ä¿¡æ¯ç³»ç»Ÿ)Sr. Data Analyst(IS)
{'https://www.liepin.com/job/1927198767.shtml'}
SAP ApplicationDevelopment Project Lead
{'https://www.liepin.com/job/1926638333.shtml'}
Machine Learning Deployment Engineer
......
......
```

æ¨¡å—åŠŸèƒ½å¾ˆå¤šï¼Œç¬¬äºŒå‘¨ä¸­çš„ç¬”è®°æœ‰æ›´å¤šçš„å™è¿°ã€‚æ›´å¤šçš„ä½¿ç”¨æ–¹å¼è¿˜è¦çœ‹[å®˜æ–¹ä¸­æ–‡æ–‡æ¡£](https://cncert.github.io/requests-html-doc-cn/#/)ã€‚

### ğŸ˜æŠ“å›¾ï¼

åŸç†å°±æ˜¯ç”¨è¯·æ±‚ï¼Œç¡®å®šåˆ°è·¯å¾„ï¼Œç„¶åå°±æ¥äº†ã€‚ç›´æ¥ä¸Šä»£ç äº†~

```
from requests_html import HTMLSession

session = HTMLSession()

r = session.get("https://cn.bing.com/images/trending")

# é€šè¿‡xpathæ‰¾åˆ°å·¥ä½œæ ‡ç­¾
items = r.html.xpath('//img/@src')

for url in items:
#     print(url)  # è·å¾—å›¾ç‰‡src url
    display(Markdown('![]({url})'.format(url=url)))  # å±•ç¤ºå›¾ç‰‡    
```

ä¸Šé¢çš„ä»£ç çˆ¬äº†å›¾å‡ºæ¥ï¼Œä¸€èˆ¬æ¥è¯´ç¼–è¾‘å™¨æ˜¯ä¸ä¼šç»™æˆ‘ä»¬é¢„è§ˆçš„ï¼Œä½†æ˜¯æˆ‘ä»¬jupyteræœ‰é»‘é­”æ³•ï¼â¬‡ï¸


### ğŸ§™â€â™‚ï¸jupyter é»‘é­”æ³•

çˆ¬ä¸œè¥¿ä¼šäº†ï¼Œçˆ¬ä¸‹æ¥è¿˜è¦æ‰“å¼€æ–‡ä»¶ä»¶æ¥æ‰¾ï¼Œå¤ªéº»çƒ¦äº†å§ï¼Ÿ

åœ¨jupyter ä¸­å°±ç›´æ¥å°±èƒ½çœ‹çˆ¬çš„å›¾ï¼

çœ‹çœ‹jupyterçš„é»‘é­”æ³•ï¼

```
# htmlå›¾
from IPython.core.display import display, HTML
display(HTML('<img src="https://httpstatusdogs.com/img/418.jpg" alt="">'))

# markdownå›¾
from IPython.core.display import display, Markdown
display(Markdown('![](https://httpstatusdogs.com/img/418.jpg)'))

## æ³¨æ„çœ‹displayåçš„æ ¼å¼

```

### ğŸ”ä¸‹è½½å›¾

ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬çˆ¬è™«ï¼Œç”¨é»‘é­”æ³•çœ‹çœ‹é¢„è§ˆå°±å¥½äº†ï¼Œç¡®å®šæ˜¯æƒ³è¦çš„å†…å®¹åï¼Œæˆ‘ä»¬ç›´æ¥ä¸‹è½½æ‰€æœ‰ï¼Œæ€ä¹ˆæï¼Ÿ

```
!mkdir img   #å…ˆæä¸ªæ–‡ä»¶å¤¹


import requests
import shutil

for imgfilename in list_img_src:
    path = urllib.parse.urljoin( url_base, imgfilename)
    
    resp = requests.get(path, stream=True)
    if r.status_code == 200:
        with open(imgfilename, mode="wb") as f:    # mode = write binary
            resp.raw.decode_content = True
            shutil.copyfileobj(resp.raw, f) 
    del resp
            
```

### ğŸ‘¨â€ğŸš€çˆ¬è±†ç“£

è±†ç“£æœ‰åçˆ¬æœºåˆ¶ï¼Œä¸ºäº†ä¸è®©ä»–è§‰å¾—æˆ‘ä»¬æ˜¯çˆ¬è™«ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾—ä¼ªè£…ä¸€ä¸‹ã€‚å¸¦ä¸Šæˆ‘ä»¬çš„èº«ä»½å»è®¿é—®ã€‚

é€šå¸¸æˆ‘ä»¬ä¼šåœ¨è¯·æ±‚å¤´ï¼ˆheadersï¼‰ä¸­å¸¦ä¸ŠUAä¹‹ç±»çš„ä¿¡æ¯ï¼ˆæœ‰äº›è¦ç™»å½•çš„è¿˜è¦å¸¦ä¸Šcookieï¼‰å»è®¿é—®ï¼Œè¿™æ ·ä»–å°±ä¸çŸ¥é“æˆ‘ä»¬æ˜¯çˆ¬è™«äº†ï¼Œ

```
import requests
from urllib import parse
_headers_ = {
        "Accept": "text/plain, */*; q=0.01", 
        "Connection": "keep-alive",
        "Host" : "movie.douban.com", 
        "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3250.0 Iron Safari/537.36",
}
s = requests.Session()
u = URL_src['è±†ç“£ç”µå½±æ’è¡Œæ¦œ']
r = s.get(u, headers=_headers_)


```

æå®šã€‚

---


#### ğŸ”§é™„åŠ ç¯‡

è¯¾ä¸Šæœ‰è®²å»çˆ¬å®˜ç½‘çš„å¸¦â€œæ–‡å­¦ä¸ä¼ åª’å­¦é™¢â€çš„æ‰€æœ‰æ–°é—»ï¼Œå°è¯•äº†ä¸€ä¸‹ï¼Œé—®é¢˜å€’ä¸å¤§ã€‚

åœ¨å¤„ç†ç¿»é¡µæ—¶ï¼Œè§‚å¯Ÿurlå’Œheaderså‘ç°ï¼Œç¿»é¡µæ˜¯é€šè¿‡urlçš„åç¼€æ”¹å˜æ¥å®ç°çš„ï¼Œæ‰€ä»¥ç”¨äº†ä¸€ä¸ª.format()å‡½æ•°å»å¤„ç†urlã€‚

```
from requests_html import HTMLSession
import pandas as pd
session = HTMLSession()
items=[]
times=[]

for i in range(1,8):
    url="http://www.nfu.edu.cn/index.php/home/article/search/keyword/%E6%96%87%E5%AD%A6%E4%B8%8E%E4%BC%A0%E5%AA%92%E5%AD%A6%E9%99%A2/p/{}.html".format(i)

    r = session.get(url)

    item = r.html.xpath('//li/div/a/@title')
    time = r.html.xpath('//li/font/text()')
#     print(item) 

#è·å–æ‰€æœ‰æ ‡é¢˜ä»¥åŠæ—¶é—´
    for a in item:
        items.append(a)
    for a in time:
        times.append(a)
# print(items)
# print(times)
    df = pd.DataFrame(
        {
            "æ ‡é¢˜":items,
            "æ—¶é—´":times,
        })
df

```

---

[ğŸ“¦ä»£ç è¿è¡Œæºç ](https://github.com/Autumnhui/Learn_PythonWebDM/blob/master/Record%20of%20Learing/week01/week01.ipynb)














